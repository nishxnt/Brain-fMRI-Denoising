{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline-UNet 3D – whole-brain visualisation\n",
    "\n",
    "This notebook crops a 32³ patch from a chosen fMRI run,\n",
    "runs the trained **UNet-3D** checkpoint and plots *noisy vs denoised* slices.\n",
    "\n",
    "> **Tip** – set `nii_path` to any NIfTI you’ve uploaded to Colab / workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install deps (Colab) – skip if you’re on your local env\n",
    "!pip -q install nibabel zarr torchmetrics matplotlib pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone the repo & add to PYTHONPATH (Colab)\n",
    "!git clone --depth 1 https://github.com/nishxnt/fmri_project.git || true\n",
    "import sys, pathlib\n",
    "sys.path.append('/content/fmri_project')  # adjust if path differs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np, matplotlib.pyplot as plt, nibabel as nib\n",
    "from fmri_project.models.unet3d import UNet3D\n",
    "from fmri_project.kim_dataset.mask import compute_mask\n",
    "from fmri_project.kim_dataset.normalize import zscore\n",
    "\n",
    "### ---- user paths -------------------------------------------------\n",
    "nii_path = \"/content/sub-01_ses-1_task-motor_run-1_bold.nii.gz\"  # change!\n",
    "ckpt  = \"/content/unet_baseline_best.pt\"                          # change if elsewhere\n",
    "#####################################################################\n",
    "\n",
    "img    = nib.load(nii_path)\n",
    "vol_np = img.get_fdata(dtype=\"float32\")          # (X,Y,Z,T)\n",
    "vol_np = np.moveaxis(vol_np, -1, 0)               # (T,X,Y,Z)\n",
    "mask_np = compute_mask(img)\n",
    "vol_np  = zscore(vol_np, mask_np)\n",
    "\n",
    "# small demo crop 32×32×32 around the centre\n",
    "xs = ys = zs = slice(40,72)\n",
    "crop_np = vol_np[:, xs, ys, zs]                   # (T,32,32,32)\n",
    "\n",
    "# (N,C,D,H,W) for UNet: (1,16,32,32,32)\n",
    "patch = torch.from_numpy(crop_np).permute(1,0,2,3)  # (32, T,32,32)\n",
    "patch = patch.unsqueeze(0).unsqueeze(1).float()\n",
    "\n",
    "net = UNet3D(in_ch=16, out_ch=16, features=16).cpu()\n",
    "net.load_state_dict(torch.load(ckpt, map_location=\"cpu\"))\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    den = net(patch).cpu().squeeze()              # (16,T,32,32)\n",
    "\n",
    "# choose t=0, axial slice z=16\n",
    "t, z = 0, 16\n",
    "noisy = crop_np[t, :, :, z]\n",
    "den2d = den[0, t, :, :, z].numpy()\n",
    "clean = vol_np[t, xs, ys, z]                     # just baseline comparison\n",
    "\n",
    "fig, axs = plt.subplots(1,3, figsize=(9,3))\n",
    "for ax, img, title in zip(axs, [noisy, den2d, clean], [\"Noisy\",\"Denoised\",\"Clean\"]):\n",
    "    ax.imshow(img, cmap=\"gray\", vmin=-2, vmax=2)\n",
    "    ax.set_title(title); ax.axis(\"off\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
